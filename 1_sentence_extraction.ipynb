{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad46eca",
   "metadata": {},
   "source": [
    "# Extracting sentences using MatBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cb1f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os\n",
    "# import sys\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# import warnings\n",
    "# from torch.optim import AdamW\n",
    "# from transformers import get_scheduler\n",
    "# from transformers.utils import logging\n",
    "# import evaluate\n",
    "# from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "load_dotenv(dotenv_path=\".env.local\", override=True)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8a279f",
   "metadata": {},
   "source": [
    "First we load the data. Here, `corpus_all_paragraphs` should be a dictionnary with the dois as keys. Each paper is thus separated. For each entry, the dictionnary should contain a list of paragraphs in the form of dicionnaries with, at least, a 'text' entry.\n",
    "\n",
    "Schematically, the data should looks like this:\n",
    "\n",
    "{\n",
    "\n",
    "doi1: [{'text': \"Some text\"}, {'text': \"Some text\"}, {'text': \"Some text\"}],\n",
    "    \n",
    "doi2: [...],\n",
    "    \n",
    "...\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4aa1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(\"data_by_corpus/papers/all_paragraphs.json\", 'r') as f:\n",
    "    corpus_all_paragraphs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e784082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:935: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\brian\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\auto\\auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758a962b8cae46d09dc3e7ff1cc8f51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\brian\\.cache\\huggingface\\hub\\models--textattack--bert-base-uncased-SST-2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and MatBERT\n",
    "token = os.getenv(\"MATBERT_TOKEN\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\", use_auth_token=token)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\", use_auth_token=token).to(device)\n",
    "\n",
    "# Create custom sentence tokenizer with abbreviations\n",
    "punkt_param = PunktParameters()\n",
    "abbreviation = ['fig', 'al', 'e.g', 'i.e']\n",
    "punkt_param.abbrev_types = set(abbreviation)\n",
    "sentence_tokenizer = PunktSentenceTokenizer(punkt_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cdfc9a",
   "metadata": {},
   "source": [
    "Here we loop over every paper and paragraph and evaluate our MatBERT model on each sentence as tokenized by `sentence_tokenizer`. If the sentence's logit prediction is 1, then we add that to `extracted_sentences`. The latter is a list of dictionnaries with each dict containing the keys `sentences`, `doi`, and `paragraph`. `sentences` is a list of sentences extracted from `paragraph` of the paper with doi `doi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34761a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_sentences = []\n",
    "\n",
    "all_dois = list(corpus_all_paragraphs.keys())\n",
    "\n",
    "for i, doi in enumerate(all_dois):\n",
    "    for para in corpus_all_paragraphs[doi]:\n",
    "\n",
    "        # To prevent extracting the same paragraph twice\n",
    "        if len(extracted_sentences)>0:\n",
    "            if para['_id'] in [s['paragraph']['_id'] for s in extracted_sentences]:\n",
    "                continue\n",
    "\n",
    "        # Some paragraphs could be empty\n",
    "        if len(para['text'])==0:\n",
    "            continue\n",
    "\n",
    "        # Get all the sentences in the paper\n",
    "        sentences = sentence_tokenizer.tokenize(para['text'])\n",
    "\n",
    "        # Prepare sentences to pass in model (tokenize them)\n",
    "        sent_tok = tokenizer(sentences, padding=True, truncation=True, max_length=512)\n",
    "        sent_tok['sentences'] = sentences\n",
    "        test_data = Dataset.from_dict(sent_tok)\n",
    "        test_data.set_format('torch')\n",
    "        test_dataloader = DataLoader(test_data, batch_size=8)\n",
    "\n",
    "        # Evaluate model\n",
    "        predictions = []\n",
    "        model.eval()\n",
    "        for batch in test_dataloader:\n",
    "            batch_sentences = batch['sentences']\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "        # If we have at least one good sentence, add it/them to list\n",
    "        if sum(predictions)>0:\n",
    "            to_add = {}\n",
    "            to_add['sentences'] = np.array(sentences)[np.array(predictions)==1].tolist()\n",
    "            to_add['doi'] = doi\n",
    "            to_add['paragraph'] = para\n",
    "            extracted_sentences.append(to_add)\n",
    "            break\n",
    "\n",
    "with open('extracted_sentences.json', 'w') as f:\n",
    "    json.dump(extracted_sentences, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71960619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5ba62",
   "metadata": {},
   "source": [
    "# Training best BERT model\n",
    "The manually labeled data used for hyperparameter tuning of various BERT models can be found in \"data/manually_labelled_sentences.csv\". Each sentence has a label of either 1 for extraction or 0 for non extraction.Â£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c436b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738cf8e295274d95b3fd388c4b445608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pd.read_csv(\"data/manually_labelled_sentences.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84f82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a777c6-3058-4668-84f0-6855c7428923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
